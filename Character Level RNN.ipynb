{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile,sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import re as regex\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv('training_4.25')\n",
    "testdata = pd.read_csv('testing_4.25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.DataFrame(traindata['tweet'].str.split('####').tolist(), index=[traindata['user_name'], traindata['class']]).stack()\n",
    "b = b.reset_index()[[0, 'user_name','class']] \n",
    "b.columns = ['tweet', 'user_name','class']\n",
    "final_train_data = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.DataFrame(testdata['tweet'].str.split('####').tolist(), index=[testdata['user_name'], testdata['class']]).stack()\n",
    "b = b.reset_index()[[0, 'user_name','class']] \n",
    "b.columns = ['tweet', 'user_name','class']\n",
    "final_test_data = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_by_regex(tweets, regexp):\n",
    "    tweets.loc[:, \"tweet\"].replace(regexp, \"\", inplace=True)\n",
    "    return tweets\n",
    "\n",
    "def remove_urls(tweets):\n",
    "    return remove_by_regex(tweets, regex.compile(r\"http.?://[^\\s]+[\\s]?\"))\n",
    "\n",
    "def remove_na(tweets):\n",
    "    return tweets[tweets[\"tweet\"] != \"Not Available\"] \n",
    "\n",
    "def remove_empty(tweets):\n",
    "    return tweets[tweets[\"tweet\"] != \" \"]\n",
    "\n",
    "def remove_special_chars(tweets):  # it unrolls the hashtags to normal words\n",
    "    for remove in map(lambda r: regex.compile(regex.escape(r)), [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\",\n",
    "                                                                 \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                                                                 \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "                                                                 \"!\", \"?\", \".\", \"'\",\n",
    "                                                                 \"--\", \"---\", \"#\"]):\n",
    "        tweets.loc[:, \"tweet\"].replace(remove, \"\", inplace=True)\n",
    "    return tweets\n",
    "\n",
    "def remove_usernames(tweets):\n",
    "    return remove_by_regex(tweets, regex.compile(r\"@[^\\s]+[\\s]?\"))\n",
    "\n",
    "def remove_numbers(tweets):\n",
    "    return remove_by_regex(tweets, regex.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = final_train_data\n",
    "traindata = remove_urls(traindata)\n",
    "traindata = remove_na(traindata)\n",
    "traindata= remove_empty(traindata)\n",
    "traindata = remove_usernames(traindata)\n",
    "traindata = remove_special_chars(traindata)\n",
    "traindata = remove_numbers(traindata)\n",
    "#traindata.cleanup(TwitterCleanuper())\n",
    "\n",
    "x = traindata.query('tweet != \"\"')\n",
    "x.shape\n",
    "traindata=x\n",
    "traindata = traindata[traindata.tweet.str.split(' ').str.len() > 1]\n",
    "traindata=traindata.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = final_test_data\n",
    "testdata = remove_urls(testdata)\n",
    "testdata = remove_na(testdata)\n",
    "testdata= remove_empty(testdata)\n",
    "testdata = remove_usernames(testdata)\n",
    "testdata = remove_special_chars(testdata)\n",
    "testdata = remove_numbers(testdata)\n",
    "#testdata.cleanup(TwitterCleanuper())\n",
    "\n",
    "x = testdata.query('tweet != \"\"')\n",
    "x.shape\n",
    "testdata=x\n",
    "testdata = testdata[testdata.tweet.str.split(' ').str.len() > 1]\n",
    "testdata=testdata.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39979, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9992, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tokenize, batchify, and copy state functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "        # Tokenize file content\n",
    "    ids = torch.ByteTensor(len(tweet.encode()))\n",
    "    token = 0\n",
    "    for char in tweet.encode():\n",
    "        ids[token] = char\n",
    "        token += 1\n",
    "\n",
    "    return ids\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    nbatch = data.size(0) // bsz\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data   \n",
    "\n",
    "def copy_state(state):\n",
    "    if isinstance(state, tuple):\n",
    "    \treturn (Variable(state[0].data), Variable(state[1].data))\n",
    "    else:\n",
    "    \treturn Variable(state.data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, features, cls_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_size = 256\n",
    "        self.embedding = nn.Embedding(features, self.hidden_size)\n",
    "        self.rnn1 = nn.GRU(input_size=features,\n",
    "                            hidden_size=self.hidden_size)\n",
    "        self.dense1 = nn.Linear(self.hidden_size, cls_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.rnn1(x, hidden)\n",
    "        #output = self.dense1(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(1, batch_size, self.hidden_size).zero_())\n",
    "\n",
    "def var(x):\n",
    "    x = Variable(x)\n",
    "    return x\n",
    "\n",
    "embed = nn.Embedding(256, 50)    \n",
    "embed_optimizer = optim.SGD(embed.parameters(), lr=0.005)\n",
    "\n",
    "model = Net(features=50, cls_size=256)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "hiddens=[]\n",
    "y=[]\n",
    "rnn = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7fae2c440888>\n",
      "0 0 0.05487421094756765\n",
      "0 iterations ran, time elapsed= 0.012838101387023926\n",
      "0 10000 4.122448936878376\n",
      "10000 iterations ran, time elapsed= 17.23893296321233\n",
      "0 20000 4.141173252684175\n",
      "20000 iterations ran, time elapsed= 33.27216882705689\n",
      "0 30000 4.139236901346705\n",
      "30000 iterations ran, time elapsed= 49.798169505596164\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    hiddenCounter=0\n",
    "    model.train()\n",
    "    batch_size = 1\n",
    "    hidden_init = model.init_hidden(1)\n",
    "    loss_avg = 0\n",
    "    t0=time.time()\n",
    "    for s in range(traindata.shape[0]):\n",
    "        model.zero_grad()\n",
    "        embed_optimizer.zero_grad()\n",
    "        hidden = hidden_init\n",
    "        loss =0\n",
    "        tweet=traindata.loc[s].tweet\n",
    "        emb = tokenize(tweet)\n",
    "        embLen=len(emb)\n",
    "        if (embLen>1):\n",
    "            for i in range(embLen): \n",
    "                character1=Variable((torch.Tensor([emb[i]])).long())\n",
    "                character1=embed(character1)\n",
    "                character1=character1.view(1,1,50)\n",
    "\n",
    "                output, hidden = model(character1, var(hidden.data))\n",
    "                output=output.view(1,256)\n",
    "\n",
    "                if i+1 < embLen: \n",
    "                    character2=Variable((torch.Tensor([emb[i+1]])).long())\n",
    "                    loss += criterion(output, character2)\n",
    "            #print (loss)\n",
    "            loss.backward()\n",
    "            tempHidden=hidden.view(256)\n",
    "            tempHidden=list(tempHidden.data.numpy())\n",
    "            hiddens.append(tempHidden)\n",
    "            y.append(traindata['class'].loc[s])\n",
    "            hidden_init = copy_state(hidden)\n",
    "            optimizer.step()\n",
    "            embed_optimizer.step()\n",
    "            loss_avg = .99*loss_avg + .01*loss.data[0]/embLen\n",
    "\n",
    "        if s % 10000 == 0:\n",
    "            print(epoch, s, loss_avg)\n",
    "            print (str(s)+' iterations ran, ''time elapsed= '+str((time.time()-t0)/60))\n",
    "\n",
    "for epoch in range(1):\n",
    "    print(model.parameters())\n",
    "    hiddens=[]\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = '4.25_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "loaded_model = joblib.load(open('4.25_model.sav',\"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training Data from RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csvfile='trainHidden_4.25.csv'\n",
    "with open(csvfile, \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(hiddens)\n",
    "\n",
    "outfile = open('trainY_4.25.csv','w')\n",
    "out = csv.writer(outfile)\n",
    "out.writerows(map(lambda x: [x], y))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['control' 'depression' 'ptsd']\n"
     ]
    }
   ],
   "source": [
    "crisisEncoder = LabelEncoder()\n",
    "y = crisisEncoder.fit_transform(y)\n",
    "print (crisisEncoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logisitic regression average accuracy :  0.844167335719\n"
     ]
    }
   ],
   "source": [
    "log = cross_val_score(LogisticRegression(), hiddens, y,cv=5)\n",
    "print(\"Logisitic regression average accuracy : \",log.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive bayes average accuracy :  0.856722587982\n"
     ]
    }
   ],
   "source": [
    "nb = cross_val_score(GaussianNB(), hiddens, y,cv=5)\n",
    "print(\"Naive bayes average accuracy : \",nb.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest average accuracy :  0.870298531805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = cross_val_score(RandomForestClassifier(), hiddens, y,cv=5)\n",
    "print(\"Random forest average accuracy : \",rf.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = RandomForestClassifier()\n",
    "# clf.fit(hiddens, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra trees average accuracy :  0.900090132678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etrees = cross_val_score(ExtraTreesClassifier(), hiddens, y,cv=5)\n",
    "print(\"Extra trees average accuracy : \",etrees.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = ExtraTreesClassifier()\n",
    "forest.fit(hiddens, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = ExtraTreesClassifier()\n",
    "logreg.fit(hiddens, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7fad9d6db888>\n"
     ]
    }
   ],
   "source": [
    "testHiddens=[]\n",
    "testY=[]\n",
    "\n",
    "def test():\n",
    "    loaded_model.eval()\n",
    "    batch_size = 1\n",
    "    hidden_init = loaded_model.init_hidden(1)\n",
    "    loss_avg = 0\n",
    "    t0=time.time()\n",
    "    for s in range(testdata.shape[0]):\n",
    "        hidden = hidden_init\n",
    "        loss =0\n",
    "        tweet=testdata.loc[s].tweet\n",
    "        emb = tokenize(tweet)\n",
    "        embLen=len(emb)\n",
    "        if (embLen>1):\n",
    "            for i in range(embLen): \n",
    "                character1=Variable((torch.Tensor([emb[i]])).long())\n",
    "                character1=embed(character1)\n",
    "                character1=character1.view(1,1,50)\n",
    "\n",
    "                output, hidden = loaded_model(character1, var(hidden.data))\n",
    "                output=output.view(1,256)\n",
    "\n",
    "                if i+1 < embLen: \n",
    "                    character2=Variable((torch.Tensor([emb[i+1]])).long())\n",
    "                    loss += criterion(output, character2)\n",
    "            #print (loss)\n",
    "            tempHidden=hidden.view(256)\n",
    "            tempHidden=list(tempHidden.data.numpy())\n",
    "            testHiddens.append(tempHidden)\n",
    "            testY.append(testdata['class'].loc[s])\n",
    "            hidden_init = copy_state(hidden)\n",
    "            loss_avg = .99*loss_avg + .01*loss.data[0]/embLen\n",
    "            \n",
    "            \n",
    "\n",
    "for epoch in range(1):\n",
    "    print(loaded_model.parameters())\n",
    "    testHiddens=[]\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving output of testing to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csvfile='testHidden_4.25.csv'\n",
    "with open(csvfile, \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(testHiddens)\n",
    "    \n",
    "    \n",
    "outfile = open('testY_4.25.csv','w')\n",
    "out = csv.writer(outfile)\n",
    "out.writerows(map(lambda x: [x], testY))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['control' 'depression' 'ptsd']\n"
     ]
    }
   ],
   "source": [
    "crisisEncoder = LabelEncoder()\n",
    "testY = crisisEncoder.fit_transform(testY)\n",
    "print (crisisEncoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classifier on test set: 0.50\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(hiddens, y)\n",
    "y_pred = logreg.predict(testHiddens)\n",
    "print('Accuracy of classifier on test set: {:.2f}'.format(logreg.score(testHiddens, testY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classifier on test set: 0.50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "forest = ExtraTreesClassifier()\n",
    "forest.fit(hiddens, y)\n",
    "y_pred = forest.predict(testHiddens)\n",
    "print('Accuracy of classifier on test set: {:.2f}'.format(forest.score(testHiddens, testY)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae2c400d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xcm=confusion_matrix(testY, y_pred, labels=None, sample_weight=None)\n",
    "xcm=numpy.array(xcm)\n",
    "ax = sns.heatmap(xcm)\n",
    "plt.xlabel('Test Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "#plt.show()\n",
    "plt.savefig('ConfusionMatrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4953,   31,   10],\n",
       "       [2480,    8,   11],\n",
       "       [2478,   13,    8]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xcm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Classification of Characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;42m i \u001b[0;30;42m   \u001b[0;30;42m d \u001b[0;30;42m o \u001b[0;30;42m   \u001b[0;30;42m n \u001b[0;30;42m o \u001b[0;30;42m t \u001b[0;30;42m   \u001b[0;30;42m w \u001b[0;30;42m a \u001b[0;30;42m n \u001b[0;30;42m t \u001b[0;30;42m   \u001b[0;30;42m t \u001b[0;30;42m o \u001b[0;30;42m   \u001b[0;30;42m b \u001b[1;30;44m e \u001b[0;30;42m   \u001b[0;30;42m a \u001b[0;30;42m l \u001b[0;30;42m i \u001b[0;30;42m v \u001b[1;30;44m e \u001b[0;30;42m   \u001b[0;30;42m a \u001b[0;30;42m n \u001b[0;30;42m y \u001b[0;30;42m m \u001b[0;30;42m o \u001b[0;30;42m r \u001b[1;30;44m e "
     ]
    }
   ],
   "source": [
    "def visualize(data):\n",
    "    hiddenCounter=0\n",
    "    loaded_model.eval()\n",
    "    batch_size = 1\n",
    "    hidden_init = loaded_model.init_hidden(1)\n",
    "    loss_avg = 0\n",
    "    for s in range(1):\n",
    "\n",
    "        hidden = hidden_init\n",
    "        loss =0\n",
    "        tweet=data\n",
    "        chars=list(tweet)\n",
    "        emb = tokenize(tweet)\n",
    "        embLen=len(emb)\n",
    "        for i in range(embLen): \n",
    "            character1=Variable((torch.Tensor([emb[i]])).long())\n",
    "            letter=character1.data.numpy()\n",
    "            character1=embed(character1)\n",
    "            character1=character1.view(1,1,50)\n",
    "            output, hidden = loaded_model(character1, var(hidden.data))\n",
    "            \n",
    "            temp=hidden.view(256)\n",
    "            temp = temp.data.numpy()\n",
    "            temp = temp.reshape(1,-1)\n",
    "            #scales=logreg.predict_proba(temp)\n",
    "            label=forest.predict(temp)\n",
    "            if label==0: #control - green \n",
    "                text=\"\\033[0;30;42m \"+chars[i]\n",
    "                print (text, end=\" \")\n",
    "            if label==1: #depressed - red\n",
    "                text=\"\\033[1;30;41m \"+chars[i]\n",
    "                print (text, end=\" \")\n",
    "            if label==2: #ptsd-blue\n",
    "                text=\"\\033[1;30;44m \"+chars[i]\n",
    "                print (text, end=\" \")\n",
    "            \n",
    "            output=output.view(1,256)\n",
    "\n",
    "            if i+1 < embLen: \n",
    "                character2=Variable((torch.Tensor([emb[i+1]])).long())\n",
    "                loss += criterion(output, character2)\n",
    "\n",
    "        hidden_init = copy_state(hidden)\n",
    "        loss_avg = .99*loss_avg + .01*loss.data[0]/embLen\n",
    "        \n",
    "visualize('i do not want to be alive anymore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
